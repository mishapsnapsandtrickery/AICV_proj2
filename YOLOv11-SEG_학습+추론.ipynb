{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUnh_U6HeXdO"
      },
      "source": [
        "# 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2QPiA16dHwB",
        "outputId": "739b3536-df62-4d92-c735-0bf84f9a4581"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IxnHO-GgE5X",
        "outputId": "dffe33e1-05c3-4225-a6a5-88cbe5a204fd"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"1ï¸âƒ£ í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "!pip install ultralytics pycocotools tqdm -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import shutil\n",
        "import random\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ\")\n",
        "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
        "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNcEKvGmeZpc"
      },
      "source": [
        "# 2. ë°ì´í„°ì…‹ ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZdb0VnAnjZK",
        "outputId": "a3dfff20-e8b0-4aa7-e1d1-c2935d30840f"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# ===============================\n",
        "# ğŸ“‚ ì‚¬ìš©ì ì§€ì • ê²½ë¡œ ì„¤ì • (ìˆ˜ì • í•„ìˆ˜)\n",
        "# ===============================\n",
        "raw_base = Path(input(\"ğŸ“ ì´ë¯¸ì§€ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: \").strip())\n",
        "label_base = Path(input(\"ğŸ·ï¸ ë¼ë²¨ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: \").strip())\n",
        "\n",
        "# ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì\n",
        "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
        "\n",
        "# ===============================\n",
        "# 1ï¸âƒ£ ì¬ê·€ì ìœ¼ë¡œ ì´ë¯¸ì§€/ë¼ë²¨ ìˆ˜ì§‘\n",
        "# ===============================\n",
        "images = [p for p in raw_base.rglob(\"*\") if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
        "labels = [p for p in label_base.rglob(\"*.json\")] if label_base.exists() else []\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“Š ì „ì²´ ê°œìˆ˜ (ì¬ê·€ íƒìƒ‰ ê²°ê³¼)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"ğŸ–¼ï¸ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ : {len(images)}ê°œ\")\n",
        "print(f\"ğŸ·ï¸ ë¼ë²¨(JSON) íŒŒì¼ ìˆ˜ : {len(labels)}ê°œ\")\n",
        "\n",
        "# ===============================\n",
        "# 2ï¸âƒ£ íŒŒì¼ëª…(í™•ì¥ì ì œì™¸) ë§¤ì¹­ ê²€ì‚¬\n",
        "# ===============================\n",
        "img_stems = {p.stem for p in images}\n",
        "lbl_stems = {p.stem for p in labels}\n",
        "\n",
        "only_img = sorted(list(img_stems - lbl_stems))[:10]\n",
        "only_lbl = sorted(list(lbl_stems - img_stems))[:10]\n",
        "\n",
        "print(\"\\nğŸ” ë§¤ì¹­ ìƒíƒœ\")\n",
        "print(f\" - ë¼ë²¨ì´ ì—†ëŠ” ì´ë¯¸ì§€: {len(img_stems - lbl_stems)}ê°œ (ì˜ˆì‹œ: {only_img})\")\n",
        "print(f\" - ì´ë¯¸ì§€ê°€ ì—†ëŠ” ë¼ë²¨: {len(lbl_stems - img_stems)}ê°œ (ì˜ˆì‹œ: {only_lbl})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX9zX3HSf7yI"
      },
      "source": [
        "# 3. í•™ìŠµ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpEE6itlmu_j",
        "outputId": "24853427-3278-423f-8e05-5ebc0708108c"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"3ï¸âƒ£ í•™ìŠµ ì„¤ì •\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from argparse import Namespace\n",
        "import os, torch\n",
        "\n",
        "# ===============================\n",
        "# âš™ï¸ ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜ ì„¤ì •\n",
        "# ===============================\n",
        "config_path = input(\"ğŸ“„ dataset.yaml íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
        "\n",
        "args = Namespace(\n",
        "    epoch=100,\n",
        "    project=\"yolo11-downsampling\",\n",
        "    name=\"experiment\",\n",
        "    config=config_path,\n",
        "    cuda=\"0\",\n",
        "    resume=False,\n",
        "    resume_path=\"\"\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# ğŸš€ ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "# ===============================\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"âœ… ë””ë°”ì´ìŠ¤: {device}\")\n",
        "print(f\"âœ… Epochs: {args.epoch}\")\n",
        "print(f\"âœ… í”„ë¡œì íŠ¸: {args.project}/{args.name}\")\n",
        "print(f\"âœ… ë°ì´í„° ì„¤ì • íŒŒì¼: {args.config}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tbk2SRWgCcq"
      },
      "source": [
        "# 4. YOLO11 í‘œì¤€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPg-JnYumvkr",
        "outputId": "73eec7b1-0add-48c5-92db-9fffb53b3901"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"4ï¸âƒ£ YOLO11 í‘œì¤€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± (ì¬ê·€íƒìƒ‰/í™•ì¥ì ë‹¤ì¤‘/ì•ˆì „ Stratify)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import os, json, shutil, pickle\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "dataset_root = Path(\"\") # Dataset ê²½ë¡œë¥¼ ê¸°ì…í•´ì£¼ì„¸ìš”.\n",
        "raw_base = Path(\"\") # images ê²½ë¡œë¥¼ ê¸°ì…í•´ì£¼ì„¸ìš”. \n",
        "label_base = Path(\"\") #labels ê²½ë¡œë¥¼ ê¸°ì…í•´ì£¼ì„¸ìš”. \n",
        "\n",
        "# ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    (dataset_root / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
        "    (dataset_root / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ì´ë¯¸ì§€ í™•ì¥ì\n",
        "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
        "\n",
        "# 1ï¸âƒ£ ì¬ê·€ë¡œ ëª¨ë“  ì´ë¯¸ì§€ ìˆ˜ì§‘\n",
        "print(\"ğŸ“‚ ì¬ê·€ë¡œ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘...\")\n",
        "images = [p for p in raw_base.rglob(\"*\") if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
        "print(f\"âœ… ì´ {len(images)}ê°œ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
        "\n",
        "if len(images) == 0:\n",
        "    print(\"âŒ ì´ë¯¸ì§€ê°€ 0ê°œì…ë‹ˆë‹¤. ì•„ë˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "    print(\"  - raw_base ê²½ë¡œê°€ ë§ë‚˜ìš”? =>\", raw_base)\n",
        "    print(\"  - ì‹¤ì œ í™•ì¥ìê°€ .jpg ì™¸ì— .JPG/.png/.jpeg ì¸ê°€ìš”?\")\n",
        "    print(\"  - ì´ë¯¸ì§€ê°€ ë” ê¹Šì€ í•˜ìœ„í´ë”ì— ìˆë‚˜ìš”? (ì´ ì½”ë“œëŠ” ì¬ê·€ë¼ë©´ OK)\")\n",
        "    raise SystemExit\n",
        "\n",
        "# (img_path, img_file, subdir) í˜•íƒœë¡œ ì •ë¦¬\n",
        "all_image_info = [(str(p), p.name, str(p.parent.relative_to(raw_base))) for p in images]\n",
        "\n",
        "# 2ï¸âƒ£ ë¼ë²¨ ì¶”ì¶œ (shapes ë¹„ë©´ Normalë¡œ ì¸ì‹)\n",
        "print(\"\\nğŸ” ë¼ë²¨ ì¶”ì¶œ ì¤‘...\")\n",
        "all_image_paths, all_labels, all_label_paths = [], [], []\n",
        "missing_label = 0\n",
        "\n",
        "for img_path, img_file, subdir in tqdm(all_image_info, desc=\"ë¼ë²¨ ë¡œë”©\"):\n",
        "    # labels/<subdir>/<same_name>.json ì„ ê¸°ëŒ€\n",
        "    if subdir == \".\":\n",
        "        json_path = label_base / (Path(img_file).with_suffix(\".json\").name)\n",
        "    else:\n",
        "        json_path = label_base / subdir / (Path(img_file).with_suffix(\".json\").name)\n",
        "\n",
        "    if json_path.exists():\n",
        "        try:\n",
        "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # âœ… í•µì‹¬ ìˆ˜ì •: shapesê°€ ë¹„ë©´ Normalë¡œ ê°„ì£¼\n",
        "            if isinstance(data, dict):\n",
        "                shapes = data.get(\"shapes\", [])\n",
        "                if len(shapes) == 0:\n",
        "                    label = \"Normal\"  # ì •ìƒ(ê°ì²´ ì—†ìŒ)\n",
        "                else:\n",
        "                    label = shapes[0].get(\"label\", \"ì •ë³´ë¶€ì¡±\")\n",
        "            else:\n",
        "                label = \"ì •ë³´ë¶€ì¡±\"\n",
        "\n",
        "        except Exception:\n",
        "            label = \"ì •ë³´ë¶€ì¡±\"\n",
        "    else:\n",
        "        label = \"ì •ë³´ë¶€ì¡±\"\n",
        "        missing_label += 1\n",
        "        json_path = None  # ë³µì‚¬ ì‹œ ìŠ¤í‚µ\n",
        "\n",
        "    all_image_paths.append(img_path)\n",
        "    all_labels.append(label)\n",
        "    all_label_paths.append(str(json_path) if json_path else None)\n",
        "\n",
        "print(f\"âœ… ë¼ë²¨ ë¡œë“œ ì™„ë£Œ (ì´ {len(all_labels)}ê°œ) / ë¼ë²¨ ëˆ„ë½: {missing_label}ê°œ\")\n",
        "\n",
        "# 3ï¸âƒ£ Stratify ì—¬ë¶€ íŒë‹¨\n",
        "cls_counts = Counter(all_labels)\n",
        "can_stratify = len(cls_counts) >= 2 and min(cls_counts.values()) >= 2\n",
        "\n",
        "train_ratio, val_ratio, test_ratio = 0.8, 0.1, 0.1\n",
        "test_size = 1 - train_ratio\n",
        "val_size = val_ratio / (val_ratio + test_ratio)\n",
        "\n",
        "print(\"\\nğŸ“Š ë°ì´í„° ë¶„í•  ì¤‘...\")\n",
        "if can_stratify:\n",
        "    strat = all_labels\n",
        "    print(f\"  - Stratify ì ìš© (í´ë˜ìŠ¤ ë¶„í¬: {dict(cls_counts)})\")\n",
        "else:\n",
        "    strat = None\n",
        "    print(f\"  - Stratify ë¯¸ì ìš© (í´ë˜ìŠ¤ ë¶„í¬: {dict(cls_counts)})\")\n",
        "\n",
        "train_paths, temp_paths, train_labels, temp_labels, train_lblpaths, temp_lblpaths = train_test_split(\n",
        "    all_image_paths, all_labels, all_label_paths,\n",
        "    test_size=test_size, stratify=strat, random_state=42\n",
        ")\n",
        "\n",
        "if can_stratify:\n",
        "    strat2 = temp_labels\n",
        "else:\n",
        "    strat2 = None\n",
        "\n",
        "val_paths, test_paths, val_labels, test_labels, val_lblpaths, test_lblpaths = train_test_split(\n",
        "    temp_paths, temp_labels, temp_lblpaths,\n",
        "    test_size=(1 - val_size), stratify=strat2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"  - Train: {len(train_paths)}ê°œ\")\n",
        "print(f\"  - Val  : {len(val_paths)}ê°œ\")\n",
        "print(f\"  - Test : {len(test_paths)}ê°œ\")\n",
        "\n",
        "# 4ï¸âƒ£ ë³µì‚¬\n",
        "print(\"\\nğŸ”„ ì´ë¯¸ì§€ ë° ë¼ë²¨ ë³µì‚¬ ì¤‘...\")\n",
        "\n",
        "def copy_split(paths, lblpaths, split):\n",
        "    for p, lp in tqdm(list(zip(paths, lblpaths)), desc=f\"{split} ì„¸íŠ¸ ë³µì‚¬\", total=len(paths)):\n",
        "        filename = Path(p).name\n",
        "        dst_img = dataset_root / \"images\" / split / filename\n",
        "        shutil.copy2(p, dst_img)\n",
        "        if lp and Path(lp).exists():\n",
        "            dst_json = dataset_root / \"labels\" / split / (Path(filename).with_suffix(\".json\").name)\n",
        "            shutil.copy2(lp, dst_json)\n",
        "\n",
        "copy_split(train_paths, train_lblpaths, \"train\")\n",
        "copy_split(val_paths,   val_lblpaths,   \"val\")\n",
        "copy_split(test_paths,  test_lblpaths,  \"test\")\n",
        "\n",
        "print(\"\\nâœ… Stratify ê¸°ë°˜(ê°€ëŠ¥ ì‹œ) ë¶„í•  + ë³µì‚¬ ì™„ë£Œ\")\n",
        "\n",
        "# 5ï¸âƒ£ ë§¤í•‘ ì •ë³´ ì €ì¥\n",
        "image_to_subdir = {}\n",
        "for paths, split in [(train_paths, \"train\"), (val_paths, \"val\"), (test_paths, \"test\")]:\n",
        "    for p in paths:\n",
        "        p = Path(p)\n",
        "        subdir = str(p.parent.relative_to(raw_base))\n",
        "        image_to_subdir[f\"{split}/{p.name}\"] = subdir\n",
        "\n",
        "with open(dataset_root / \"image_to_subdir.pkl\", \"wb\") as f:\n",
        "    pickle.dump(image_to_subdir, f)\n",
        "\n",
        "print(\"âœ… ë§¤í•‘ ì •ë³´ ì €ì¥ ì™„ë£Œ (image_to_subdir.pkl)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dvZvLlQgLy0"
      },
      "source": [
        "# 5. JSON â†’ YOLO11 ë¼ë²¨ ë³€í™˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDiqzMHTmwOB",
        "outputId": "0264e973-1147-4c91-998e-e01c3c841de6"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"5ï¸âƒ£ JSON â†’ YOLO11 ë¼ë²¨ ë³€í™˜\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import os, json, pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# JSONì´ ì•„ì˜ˆ ì—†ì„ ë•Œë„ Normal(ë¹ˆ .txt)ë¡œ ë§Œë“¤ì§€ ì—¬ë¶€\n",
        "TREAT_JSON_MISSING_AS_NORMAL = True\n",
        "\n",
        "# í´ë˜ìŠ¤ ë§¤í•‘ (ì´ë¯¸ ìœ„ ì…€ì—ì„œ ì •ì˜í•œ ê±¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ë¨)\n",
        "class_name_to_number = {\n",
        "    'ConcreteCrack': '0',\n",
        "    'Spalling': '1',\n",
        "    'Efflorescene': '2',\n",
        "    'Leak': '3',\n",
        "    'Exposure': '4',\n",
        "    'AspaltCrack': '5',\n",
        "    'Pothole': '6',\n",
        "    'SteelDamage': '7',\n",
        "}\n",
        "\n",
        "def process_coordinates(points, img_width=512, img_height=512):\n",
        "    processed_coords = []\n",
        "    for point in points:\n",
        "        if isinstance(point, list) and len(point) >= 2:\n",
        "            x = float(point[0]) / img_width\n",
        "            y = float(point[1]) / img_height\n",
        "            processed_coords.append(f\"{x:.6f} {y:.6f}\")\n",
        "    return ' '.join(processed_coords)\n",
        "\n",
        "def convert_json_to_yolo(json_path, output_path):\n",
        "    \"\"\"\n",
        "    - shapesê°€ ë¹„ë©´ Normal(ê°ì²´ 0ê°œ)ë¡œ ê°„ì£¼ â†’ ë¹ˆ .txt ìƒì„±\n",
        "    - shapesê°€ ìˆìœ¼ë©´ ê° polygonì„ YOLO ì„¸ê·¸ í˜•ì‹ìœ¼ë¡œ ê¸°ë¡\n",
        "    - ì„±ê³µ ì‹œ True, ë³€í™˜í•  ê°ì²´ ì—†ìœ¼ë©´ False ë°˜í™˜\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        txt_output = []\n",
        "        img_width  = data.get('imageWidth',  512)\n",
        "        img_height = data.get('imageHeight', 512)\n",
        "        shapes     = data.get('shapes', [])\n",
        "\n",
        "        # â˜… Normal(ê°ì²´ ì—†ìŒ): shapesê°€ ë¹„ë©´ ë¹ˆ .txt ìƒì„± í›„ True ë°˜í™˜\n",
        "        if not shapes:\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"\")   # ë‚´ìš© ì—†ëŠ” .txt\n",
        "            return True\n",
        "\n",
        "        for shape in shapes:\n",
        "            label = shape.get('label', 'Unknown')\n",
        "            class_number = class_name_to_number.get(label, None)\n",
        "            if class_number is None:\n",
        "                continue\n",
        "\n",
        "            points = shape.get('points', [])\n",
        "            if not points or len(points) < 3:\n",
        "                continue\n",
        "\n",
        "            coordinates = process_coordinates(points, img_width, img_height)\n",
        "            if coordinates:\n",
        "                txt_output.append(f\"{class_number} {coordinates}\")\n",
        "\n",
        "        if txt_output:\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                f.write('\\n'.join(txt_output))\n",
        "            return True\n",
        "\n",
        "        # ê°ì²´ê°€ ì œëŒ€ë¡œ ê¸°ë¡ë˜ì§€ ì•Šì€ ê²½ìš°\n",
        "        return False\n",
        "\n",
        "    except Exception:\n",
        "        # JSON íŒŒì‹± ì‹¤íŒ¨ ë“±\n",
        "        return False\n",
        "\n",
        "# ===== ë§¤í•‘ ì •ë³´ ë¡œë“œ =====\n",
        "with open(f\"{dataset_root}/image_to_subdir.pkl\", 'rb') as f:\n",
        "    image_to_subdir = pickle.load(f)\n",
        "\n",
        "label_base = \"\" # labels ê²½ë¡œ ê¸°ì…í•´ì£¼ì„¸ìš”. \n",
        "\n",
        "print(\"ğŸ”„ ë¼ë²¨ ë³€í™˜ ì¤‘...\")\n",
        "conversion_stats = {'train': 0, 'val': 0, 'test': 0}\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    img_folder   = f\"{dataset_root}/images/{split}\"\n",
        "    label_folder = f\"{dataset_root}/labels/{split}\"\n",
        "\n",
        "    # ì´ë¯¸ì§€ ëª©ë¡(.jpgë§Œ) â†’ í•„ìš”í•˜ë©´ í™•ì¥ì ì¶”ê°€ ê°€ëŠ¥\n",
        "    image_files_in_split = [f for f in os.listdir(img_folder) if f.lower().endswith('.jpg')]\n",
        "\n",
        "    for img_file in tqdm(image_files_in_split, desc=f\"{split} ë¼ë²¨ ë³€í™˜\"):\n",
        "        # ì›ë˜ í•˜ìœ„ í´ë”(ë ˆì´ë¸” ìœ„ì¹˜) ì¶”ì \n",
        "        subdir = image_to_subdir.get(f\"{split}/{img_file}\", None)\n",
        "        if subdir is None:\n",
        "            continue\n",
        "\n",
        "        img_name = os.path.splitext(img_file)[0]\n",
        "        json_path = os.path.join(label_base, subdir, f\"{img_name}.json\")\n",
        "        txt_path  = os.path.join(label_folder, f\"{img_name}.txt\")\n",
        "\n",
        "        if os.path.exists(json_path):\n",
        "            if convert_json_to_yolo(json_path, txt_path):\n",
        "                conversion_stats[split] += 1\n",
        "        else:\n",
        "            # â˜… JSON ìì²´ê°€ ì—†ì„ ë•Œë„ Normal(ë¹ˆ .txt)ë¡œ ì·¨ê¸‰í•˜ê³  ì‹¶ë‹¤ë©´:\n",
        "            if TREAT_JSON_MISSING_AS_NORMAL:\n",
        "                with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "                    f.write(\"\")  # ë¹ˆ .txt ìƒì„±\n",
        "                conversion_stats[split] += 1\n",
        "            # ì•„ë‹ˆë©´ ìŠ¤í‚µ\n",
        "\n",
        "print(\"\\nâœ… ë¼ë²¨ ë³€í™˜ ì™„ë£Œ\")\n",
        "for split, count in conversion_stats.items():\n",
        "    total = len([f for f in os.listdir(f\"{dataset_root}/images/{split}\") if f.lower().endswith('.jpg')])\n",
        "    rate = (count / total * 100) if total > 0 else 0\n",
        "    print(f\"  - {split}: {count}/{total}ê°œ ({rate:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmOJbuJxgQoN"
      },
      "source": [
        "# 6. YAML ì„¤ì • íŒŒì¼ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4FfdGR3T9of",
        "outputId": "06de9558-8f1e-42c7-e1fb-d7054ca0e7b9"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"6ï¸âƒ£ YAML ì„¤ì • íŒŒì¼ ìƒì„±\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "dataset_root = \"\" # ë°ì´í„°ì…‹ ê²½ë¡œ ê¸°ì…í•´ì£¼ì„¸ìš”. \n",
        "\n",
        "yaml_content = f\"\"\"# YOLO11 Segmentation Dataset\n",
        "path: {dataset_root}\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "\n",
        "names:\n",
        "  0: ConcreteCrack\n",
        "  1: Spalling\n",
        "  2: Efflorescene\n",
        "  3: Leak\n",
        "  4: Exposure\n",
        "  5: AspaltCrack\n",
        "  6: Pothole\n",
        "  7: SteelDamage\n",
        "\n",
        "nc: 8\n",
        "\"\"\"\n",
        "\n",
        "with open(args.config, 'w', encoding='utf-8') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(f\"âœ… YAML íŒŒì¼ ìƒì„±: {args.config}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF3gt7tXgcHS"
      },
      "source": [
        "# 7. ë°ì´í„°ì…‹ ê²€ì¦"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "ajM-Hug2X69a",
        "outputId": "5e17a5ee-18cc-46a5-be32-0bb82040d202"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"7ï¸âƒ£ ë°ì´í„°ì…‹ ê²€ì¦\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "dataset_root = Path(\"\") # dataset ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. \n",
        "raw_base = Path(\"\") # images ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. \n",
        "label_base = Path(\"\") # labels ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. \n",
        "\n",
        "# í´ë˜ìŠ¤ ë§¤í•‘ (ì´ë¯¸ ìœ„ ì…€ì—ì„œ ì •ì˜í•œ ê±¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ë¨)\n",
        "class_name_to_number = {\n",
        "    'ConcreteCrack': '0',\n",
        "    'Spalling': '1',\n",
        "    'Efflorescene': '2',\n",
        "    'Leak': '3',\n",
        "    'Exposure': '4',\n",
        "    'AspaltCrack': '5',\n",
        "    'Pothole': '6',\n",
        "    'SteelDamage': '7',\n",
        "}\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    img_folder = f\"{dataset_root}/images/{split}\"\n",
        "    label_folder = f\"{dataset_root}/labels/{split}\"\n",
        "\n",
        "    img_count = len([f for f in os.listdir(img_folder) if f.endswith('.jpg')])\n",
        "    label_count = len([f for f in os.listdir(label_folder) if f.endswith('.txt')])\n",
        "\n",
        "    match_rate = (label_count / img_count * 100) if img_count > 0 else 0\n",
        "    print(f\"{split:6s}: ì´ë¯¸ì§€ {img_count:4d}ê°œ, ë¼ë²¨ {label_count:4d}ê°œ (ë§¤ì¹­ë¥ : {match_rate:.1f}%)\")\n",
        "\n",
        "sample_labels = glob.glob(f'{dataset_root}/labels/train/*.txt')\n",
        "if sample_labels:\n",
        "    with open(sample_labels[0], 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        print(f\"\\nğŸ“ ìƒ˜í”Œ ë¼ë²¨: {os.path.basename(sample_labels[0])}\")\n",
        "        print(f\"   - ê°ì²´ ìˆ˜: {len(lines)}\")\n",
        "        if lines:\n",
        "            parts = lines[0].strip().split()\n",
        "            print(f\"   - í´ë˜ìŠ¤: {parts[0]}, ì¢Œí‘œì : {(len(parts)-1)//2}ê°œ\")\n",
        "\n",
        "print(f\"\\nğŸ“Š í´ë˜ìŠ¤ë³„ í†µê³„:\")\n",
        "class_counts = {name: 0 for name in class_name_to_number.keys()}\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    for txt_file in os.listdir(f\"{dataset_root}/labels/{split}\"):\n",
        "        if txt_file.endswith('.txt'):\n",
        "            with open(f\"{dataset_root}/labels/{split}/{txt_file}\", 'r') as f:\n",
        "                for line in f:\n",
        "                    class_id = line.strip().split()[0]\n",
        "                    for name, cid in class_name_to_number.items():\n",
        "                        if cid == class_id:\n",
        "                            class_counts[name] += 1\n",
        "                            break\n",
        "\n",
        "total_objects = sum(class_counts.values())\n",
        "for class_name, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    if count > 0:\n",
        "        percentage = (count / total_objects * 100)\n",
        "        print(f\"  {class_name:15s}: {count:4d}ê°œ ({percentage:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBeien9jghP3"
      },
      "source": [
        "# 8. YOLO11 ëª¨ë¸ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvh4NMxbm4GH",
        "outputId": "91812556-4761-45ac-a7fe-0a7fa562a58e"
      },
      "outputs": [],
      "source": [
        "import glob, os\n",
        "\n",
        "dataset_root = Path(\"\") # dataset ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. \n",
        "\n",
        "# dataset_rootëŠ” ì½”ë“œì—ì„œ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆìŒ\n",
        "for cache_file in glob.glob(f\"{dataset_root}/**/*.cache\", recursive=True):\n",
        "    print(f\"ğŸ—‘ ì‚­ì œ: {cache_file}\")\n",
        "    os.remove(cache_file)\n",
        "\n",
        "# T4 ê¸°ì¤€, 15ë¶„ ë‚´ì™¸ë¡œ í•™ìŠµ ì†Œìš”\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"8ï¸âƒ£ YOLO11 ëª¨ë¸ í•™ìŠµ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "MODEL = \"yolo11m-seg.pt\" # ì›í•˜ëŠ” ëª¨ë¸ë¡œ ë³€ê²½í•˜ì—¬ ê¸°ì…\n",
        "\n",
        "if args.resume and os.path.exists(args.resume_path):\n",
        "    print(f\"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ: {args.resume_path}\")\n",
        "    model = YOLO(args.resume_path)\n",
        "else:\n",
        "    print(f\"ğŸ†• ìƒˆ ëª¨ë¸ ë¡œë“œ: {MODEL}\")\n",
        "    model = YOLO(MODEL)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f\"\\nğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
        "results = model.train(\n",
        "    project=args.project,\n",
        "    name=args.name,\n",
        "    data=args.config,\n",
        "    epochs=args.epoch,\n",
        "    imgsz=512,\n",
        "    batch=32, # ê¸°ì¡´ì˜ ì½”ë“œëŠ” 16ìœ¼ë¡œ í–ˆëŠ”ë°, VRAMì´ ë‚¨ì•„ì„œ 32ë¡œ ì§„í–‰í•´ë„ ê´œì°®ì•„ ë³´ì…ë‹ˆë‹¤.\n",
        "    device=device,\n",
        "    optimizer=\"auto\",\n",
        "    val=True,\n",
        "    resume=args.resume,\n",
        "    verbose=True,\n",
        "    patience=100,\n",
        "    save=True,\n",
        "    plots=True,\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“ ê²°ê³¼: {args.project}/{args.name}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so3KsRq0hzlk"
      },
      "source": [
        "# 9. ìµœì  ëª¨ë¸ ê²€ì¦"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZjAkWibn6R5",
        "outputId": "bcadcce6-789c-4156-c878-fc45a1ad89eb"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"9ï¸âƒ£ ìµœì  ëª¨ë¸ ê²€ì¦\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_model_path = f\"{args.project}/{args.name}/weights/best.pt\"\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"ğŸ“¦ ìµœì  ëª¨ë¸ ë¡œë“œ: {best_model_path}\")\n",
        "    best_model = YOLO(best_model_path)\n",
        "\n",
        "    print(\"\\nğŸ“Š ê²€ì¦ ì„¸íŠ¸ í‰ê°€...\")\n",
        "    val_results = best_model.val(\n",
        "        data=args.config,\n",
        "        split='val',\n",
        "        imgsz=512,\n",
        "        batch=16,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    print(\"\\nâœ… ê²€ì¦ ì™„ë£Œ!\")\n",
        "    print(f\"   - Box mAP50:     {val_results.box.map50:.4f}\")\n",
        "    print(f\"   - Box mAP50-95:  {val_results.box.map:.4f}\")\n",
        "    print(f\"   - Mask mAP50:    {val_results.seg.map50:.4f}\")\n",
        "    print(f\"   - Mask mAP50-95: {val_results.seg.map:.4f}\")\n",
        "else:\n",
        "    print(f\"âš ï¸  ìµœì  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {best_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChArVaYWh2AC"
      },
      "source": [
        "# 10. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì¶”ë¡ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os, glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "test_images_folder = f\"{dataset_root}/images/test\"\n",
        "test_labels_folder = f\"{dataset_root}/labels/test\"   # YOLO txt GT í´ë”\n",
        "test_images = glob.glob(os.path.join(test_images_folder, \"*.jpg\"))\n",
        "\n",
        "CLASS_NAMES = [\n",
        "    \"ConcreteCrack\",\n",
        "    \"Spalling\",\n",
        "    \"Efflorescene\",\n",
        "    \"Leak\",\n",
        "    \"Exposure\",\n",
        "    \"AspaltCrack\",\n",
        "    \"Pothole\",\n",
        "    \"SteelDamage\"\n",
        "]\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# ë³´ì¡° í•¨ìˆ˜: YOLO polygon â†’ mask ë³€í™˜\n",
        "# -----------------------------\n",
        "def yolo_txt_to_mask(txt_path, img_shape, num_classes):\n",
        "    \"\"\"YOLO polygon í˜•ì‹ txt íŒŒì¼ì„ í”½ì…€ ë§ˆìŠ¤í¬ë¡œ ë³€í™˜\"\"\"\n",
        "    h, w = img_shape[:2]\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "    if not os.path.exists(txt_path):\n",
        "        return mask\n",
        "\n",
        "    with open(txt_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) < 7:\n",
        "            continue\n",
        "        cls_id = int(parts[0])\n",
        "        coords = np.array(list(map(float, parts[1:])))\n",
        "        xs = (coords[0::2] * w).astype(np.int32)\n",
        "        ys = (coords[1::2] * h).astype(np.int32)\n",
        "        poly = np.stack([xs, ys], axis=1)\n",
        "        cv2.fillPoly(mask, [poly], color=cls_id + 1)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYrlVBNjmkch",
        "outputId": "8112172d-49be-40ff-b1b2-1983858c7c55"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# IoU ê³„ì‚°\n",
        "# -----------------------------\n",
        "ious_all = []\n",
        "ious_per_class = {cls: [] for cls in CLASS_NAMES}\n",
        "\n",
        "print(f\"ğŸ” ì´ {len(test_images)}ê°œ ì´ë¯¸ì§€ í‰ê°€ ì¤‘...\")\n",
        "\n",
        "for img_path in tqdm(test_images, desc=\"IoU ê³„ì‚°\"):\n",
        "    img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "    gt_label_path = os.path.join(test_labels_folder, f\"{img_name}.txt\")\n",
        "\n",
        "    # ì´ë¯¸ì§€ í¬ê¸°\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "    h, w, _ = img.shape\n",
        "\n",
        "    # 1ï¸âƒ£ GT mask ìƒì„±\n",
        "    gt_mask = yolo_txt_to_mask(gt_label_path, (h, w, 3), NUM_CLASSES)\n",
        "\n",
        "    # 2ï¸âƒ£ YOLO ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    results = best_model.predict(\n",
        "        source=img_path,\n",
        "        imgsz=512,\n",
        "        conf=0.25,\n",
        "        iou=0.5,\n",
        "        device=device,\n",
        "        save=False,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # 3ï¸âƒ£ ì˜ˆì¸¡ ë§ˆìŠ¤í¬ ìƒì„±\n",
        "    pred_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    if results[0].masks is not None:\n",
        "        for m, c in zip(results[0].masks.data, results[0].boxes.cls):\n",
        "            cls_id = int(c.item()) + 1\n",
        "            mask = (m.cpu().numpy() * 255).astype(np.uint8)\n",
        "            mask_resized = cv2.resize(mask, (w, h))\n",
        "            pred_mask[mask_resized > 127] = cls_id\n",
        "\n",
        "    # 4ï¸âƒ£ IoU ê³„ì‚°\n",
        "    gt_flat = gt_mask.flatten()\n",
        "    pred_flat = pred_mask.flatten()\n",
        "    iou_mean = jaccard_score(gt_flat, pred_flat, average=\"macro\", zero_division=0)\n",
        "    ious_all.append(iou_mean)\n",
        "\n",
        "    for idx, cls_name in enumerate(CLASS_NAMES, start=1):\n",
        "        gt_bin = (gt_mask == idx).astype(np.uint8)\n",
        "        pred_bin = (pred_mask == idx).astype(np.uint8)\n",
        "        if np.sum(gt_bin) == 0 and np.sum(pred_bin) == 0:\n",
        "            continue\n",
        "        iou_cls = jaccard_score(gt_bin.flatten(), pred_bin.flatten(), average=\"binary\", zero_division=0)\n",
        "        ious_per_class[cls_name].append(iou_cls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
        "# -----------------------------\n",
        "mean_iou = np.mean(ious_all)\n",
        "median_iou = np.median(ious_all)\n",
        "std_iou = np.std(ious_all)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ“Š ì„¸ê·¸ë©˜í…Œì´ì…˜ IoU ê²°ê³¼ ìš”ì•½\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ğŸ“Œ Mean IoU:   {mean_iou:.4f}\")\n",
        "print(f\"ğŸ“Œ Median IoU: {median_iou:.4f}\")\n",
        "print(f\"ğŸ“Œ Std Dev:    {std_iou:.4f}\")\n",
        "\n",
        "print(\"\\nğŸ“ˆ í´ë˜ìŠ¤ë³„ í‰ê·  IoU\")\n",
        "for cls, vals in ious_per_class.items():\n",
        "    if len(vals) > 0:\n",
        "        print(f\" - {cls:15s}: {np.mean(vals):.4f}\")\n",
        "    else:\n",
        "        print(f\" - {cls:15s}: ë°ì´í„° ì—†ìŒ\")\n",
        "\n",
        "print(\"\\nâœ… ì„¸ê·¸ë©˜í…Œì´ì…˜ IoU ê³„ì‚° ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq7gHL7Oh40P"
      },
      "source": [
        "# 11. ì¶”ë¡  ê²°ê³¼ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "3qu3lNIFh5T3",
        "outputId": "27eb4b0a-ae0c-4cc2-dc21-6c7077b08a6b"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"1ï¸âƒ£1ï¸âƒ£ ì¶”ë¡  ê²°ê³¼ ì‹œê°í™”\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "result_images = glob.glob(f\"{output_folder}/test_results/*.jpg\")[:5]\n",
        "\n",
        "if result_images:\n",
        "    fig, axes = plt.subplots(1, len(result_images), figsize=(20, 4))\n",
        "    if len(result_images) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, img_path in enumerate(result_images):\n",
        "        img = Image.open(img_path)\n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].axis('off')\n",
        "        axes[idx].set_title(os.path.basename(img_path), fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_folder}/visualization.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"âœ… ì‹œê°í™” ì €ì¥: {output_folder}/visualization.png\")\n",
        "else:\n",
        "    print(\"âš ï¸  ì¶”ë¡  ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoiIOwr9h7Cb"
      },
      "source": [
        "# 12. í•™ìŠµ ê²°ê³¼ ìš”ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "MH_4Jmfgh7t5",
        "outputId": "10c1c33d-38ec-417b-c5f3-275a68a60313"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"1ï¸âƒ£2ï¸âƒ£ í•™ìŠµ ê²°ê³¼ ìš”ì•½\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results_png = f\"{args.project}/{args.name}/results.png\"\n",
        "confusion_matrix_png = f\"{args.project}/{args.name}/confusion_matrix.png\"\n",
        "\n",
        "if os.path.exists(results_png) or os.path.exists(confusion_matrix_png):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    if os.path.exists(results_png):\n",
        "        img1 = Image.open(results_png)\n",
        "        axes[0].imshow(img1)\n",
        "        axes[0].set_title('Training Results', fontweight='bold')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "    if os.path.exists(confusion_matrix_png):\n",
        "        img2 = Image.open(confusion_matrix_png)\n",
        "        axes[1].imshow(img2)\n",
        "        axes[1].set_title('Confusion Matrix', fontweight='bold')\n",
        "        axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{args.project}/{args.name}/training_summary.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"âœ… í•™ìŠµ ìš”ì•½: {args.project}/{args.name}/training_summary.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nğŸ“‚ ì£¼ìš” ê²°ê³¼ë¬¼:\")\n",
        "print(f\"   1. ë°ì´í„°ì…‹: {dataset_root}/\")\n",
        "print(f\"   2. ìµœì  ëª¨ë¸: {best_model_path}\")\n",
        "print(f\"   3. í•™ìŠµ ê²°ê³¼: {args.project}/{args.name}/\")\n",
        "print(f\"   4. ì¶”ë¡  ê²°ê³¼: {output_folder}/test_results/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAOhCeB_oQqb"
      },
      "source": [
        "# 13. Google Drive ë§ˆìš´íŠ¸ ë° ë°±ì—…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHHVVxXToS4I",
        "outputId": "b23f41ff-425f-4300-bb58-02ce38f3ec04"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"1ï¸âƒ£3ï¸âƒ£ Google Drive ë°±ì—…\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "print(\"ğŸ”— Google Drive ë§ˆìš´íŠ¸ ì¤‘...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\")\n",
        "\n",
        "# ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "backup_root = f\"/YOLO11_Backup_{timestamp}\" # ì €ì¥í•  ê²½ë¡œë¥¼ ê¸°ì…í•´ì£¼ì„¸ìš”. \n",
        "os.makedirs(backup_root, exist_ok=True)\n",
        "\n",
        "print(f\"\\nğŸ“ ë°±ì—… ë””ë ‰í† ë¦¬: {backup_root}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvVZCca1oYV3",
        "outputId": "5c5df1f7-cd2d-4e21-af74-3dca150fb99a"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 1. í•™ìŠµ ê²°ê³¼ ë°±ì—…\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"1ï¸âƒ£ í•™ìŠµ ê²°ê³¼ ë°±ì—…\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class_name_to_number = {\n",
        "    'ConcreteCrack': '0',\n",
        "    'Spalling': '1',\n",
        "    'Efflorescene': '2',\n",
        "    'Leak': '3',\n",
        "    'Exposure': '4',\n",
        "    'AspaltCrack': '5',\n",
        "    'Pothole': '6',\n",
        "    'SteelDamage': '7',\n",
        "}\n",
        "\n",
        "\n",
        "source_project = f\"{args.project}/{args.name}\"\n",
        "dest_project = f\"{backup_root}/training_results\"\n",
        "\n",
        "if os.path.exists(source_project):\n",
        "    print(f\"ğŸ“¦ ë³µì‚¬ ì¤‘: {source_project}\")\n",
        "    shutil.copytree(source_project, dest_project, dirs_exist_ok=True)\n",
        "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {dest_project}\")\n",
        "\n",
        "    # ì£¼ìš” íŒŒì¼ í™•ì¸\n",
        "    important_files = [\n",
        "        'weights/best.pt',\n",
        "        'weights/last.pt',\n",
        "        'results.png',\n",
        "        'confusion_matrix.png',\n",
        "        'results.csv'\n",
        "    ]\n",
        "\n",
        "    print(\"\\nì£¼ìš” íŒŒì¼ í™•ì¸:\")\n",
        "    for file in important_files:\n",
        "        file_path = os.path.join(dest_project, file)\n",
        "        if os.path.exists(file_path):\n",
        "            size = os.path.getsize(file_path) / (1024**2)  # MB\n",
        "            print(f\"  âœ“ {file}: {size:.2f} MB\")\n",
        "        else:\n",
        "            print(f\"  âœ— {file}: ì—†ìŒ\")\n",
        "else:\n",
        "    print(f\"âš ï¸  í•™ìŠµ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_project}\")\n",
        "\n",
        "# ========================================\n",
        "# 2. ë°ì´í„°ì…‹ ì •ë³´ ë°±ì—… (êµ¬ì¡°ë§Œ)\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"2ï¸âƒ£ ë°ì´í„°ì…‹ ì •ë³´ ë°±ì—…\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "dataset_info_dir = f\"{backup_root}/dataset_info\"\n",
        "os.makedirs(dataset_info_dir, exist_ok=True)\n",
        "\n",
        "# YAML íŒŒì¼ ë³µì‚¬\n",
        "if os.path.exists(args.config):\n",
        "    shutil.copy2(args.config, f\"{dataset_info_dir}/dataset.yaml\")\n",
        "    print(f\"âœ… YAML íŒŒì¼ ë³µì‚¬ ì™„ë£Œ\")\n",
        "\n",
        "# ë§¤í•‘ ì •ë³´ ë³µì‚¬\n",
        "mapping_file = f\"{dataset_root}/image_to_subdir.pkl\"\n",
        "if os.path.exists(mapping_file):\n",
        "    shutil.copy2(mapping_file, f\"{dataset_info_dir}/image_to_subdir.pkl\")\n",
        "    print(f\"âœ… ë§¤í•‘ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ\")\n",
        "\n",
        "# ë°ì´í„°ì…‹ í†µê³„ ì €ì¥\n",
        "stats_file = f\"{dataset_info_dir}/dataset_statistics.txt\"\n",
        "with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\" * 60 + \"\\n\")\n",
        "    f.write(\"YOLO11 ë°ì´í„°ì…‹ í†µê³„\\n\")\n",
        "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "    f.write(f\"ìƒì„± ì‹œê°„: {timestamp}\\n\\n\")\n",
        "\n",
        "    f.write(\"ë°ì´í„° ë¶„í• :\\n\")\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        img_count = len([f for f in os.listdir(f\"{dataset_root}/images/{split}\") if f.endswith('.jpg')])\n",
        "        label_count = len([f for f in os.listdir(f\"{dataset_root}/labels/{split}\") if f.endswith('.txt')])\n",
        "        f.write(f\"  - {split:6s}: ì´ë¯¸ì§€ {img_count:4d}ê°œ, ë¼ë²¨ {label_count:4d}ê°œ\\n\")\n",
        "\n",
        "    f.write(\"\\ní´ë˜ìŠ¤ë³„ í†µê³„:\\n\")\n",
        "    class_counts = {name: 0 for name in class_name_to_number.keys()}\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for txt_file in os.listdir(f\"{dataset_root}/labels/{split}\"):\n",
        "            if txt_file.endswith('.txt'):\n",
        "                with open(f\"{dataset_root}/labels/{split}/{txt_file}\", 'r') as tf:\n",
        "                    for line in tf:\n",
        "                        parts = line.strip().split()\n",
        "                        if parts:\n",
        "                            class_id = parts[0]\n",
        "                            for name, cid in class_name_to_number.items():\n",
        "                                if cid == class_id:\n",
        "                                    class_counts[name] += 1\n",
        "                                    break\n",
        "\n",
        "    total_objects = sum(class_counts.values())\n",
        "    for class_name, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        if count > 0:\n",
        "            percentage = (count / total_objects * 100)\n",
        "            f.write(f\"  - {class_name:15s}: {count:4d}ê°œ ({percentage:5.1f}%)\\n\")\n",
        "\n",
        "    f.write(f\"\\nì´ ê°ì²´ ìˆ˜: {total_objects}ê°œ\\n\")\n",
        "\n",
        "print(f\"âœ… ë°ì´í„°ì…‹ í†µê³„ ì €ì¥ ì™„ë£Œ: dataset_statistics.txt\")\n",
        "\n",
        "# ========================================\n",
        "# 3. ì¶”ë¡  ê²°ê³¼ ë°±ì—… (ìˆëŠ” ê²½ìš°)\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"3ï¸âƒ£ ì¶”ë¡  ê²°ê³¼ ë°±ì—…\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "prediction_source = f\"{args.project}/{args.name}/predictions\"\n",
        "prediction_dest = f\"{backup_root}/predictions\"\n",
        "\n",
        "if os.path.exists(prediction_source):\n",
        "    print(f\"ğŸ“¦ ë³µì‚¬ ì¤‘: {prediction_source}\")\n",
        "    shutil.copytree(prediction_source, prediction_dest, dirs_exist_ok=True)\n",
        "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {prediction_dest}\")\n",
        "else:\n",
        "    print(f\"âš ï¸  ì¶”ë¡  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
        "\n",
        "# ========================================\n",
        "# 4. ìƒ˜í”Œ ì´ë¯¸ì§€ ë° ë¼ë²¨ ë°±ì—…\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"4ï¸âƒ£ ìƒ˜í”Œ ë°ì´í„° ë°±ì—…\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sample_dir = f\"{backup_root}/samples\"\n",
        "os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "# ê° splitì—ì„œ 5ê°œì”© ìƒ˜í”Œ ë³µì‚¬\n",
        "for split in ['train', 'val', 'test']:\n",
        "    split_sample_dir = f\"{sample_dir}/{split}\"\n",
        "    os.makedirs(f\"{split_sample_dir}/images\", exist_ok=True)\n",
        "    os.makedirs(f\"{split_sample_dir}/labels\", exist_ok=True)\n",
        "\n",
        "    img_files = [f for f in os.listdir(f\"{dataset_root}/images/{split}\") if f.endswith('.jpg')][:5]\n",
        "\n",
        "    for img_file in img_files:\n",
        "        # ì´ë¯¸ì§€ ë³µì‚¬\n",
        "        src_img = f\"{dataset_root}/images/{split}/{img_file}\"\n",
        "        dst_img = f\"{split_sample_dir}/images/{img_file}\"\n",
        "        shutil.copy2(src_img, dst_img)\n",
        "\n",
        "        # ë¼ë²¨ ë³µì‚¬\n",
        "        txt_file = img_file.replace('.jpg', '.txt')\n",
        "        src_label = f\"{dataset_root}/labels/{split}/{txt_file}\"\n",
        "        dst_label = f\"{split_sample_dir}/labels/{txt_file}\"\n",
        "\n",
        "        if os.path.exists(src_label):\n",
        "            shutil.copy2(src_label, dst_label)\n",
        "\n",
        "    print(f\"âœ… {split}: {len(img_files)}ê°œ ìƒ˜í”Œ ë³µì‚¬ ì™„ë£Œ\")\n",
        "\n",
        "# ========================================\n",
        "# 5. í•™ìŠµ ì„¤ì • ì €ì¥\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"5ï¸âƒ£ í•™ìŠµ ì„¤ì • ì €ì¥\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "config_file = f\"{backup_root}/training_config.txt\"\n",
        "with open(config_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\" * 60 + \"\\n\")\n",
        "    f.write(\"YOLOv11 í•™ìŠµ ì„¤ì •\\n\")\n",
        "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "    f.write(f\"ìƒì„± ì‹œê°„: {timestamp}\\n\\n\")\n",
        "\n",
        "    f.write(\"í•™ìŠµ íŒŒë¼ë¯¸í„°:\\n\")\n",
        "    f.write(f\"  - ëª¨ë¸: yolov11s-seg.pt\\n\")\n",
        "    f.write(f\"  - Epochs: {args.epoch}\\n\")\n",
        "    f.write(f\"  - Batch size: 16\\n\")\n",
        "    f.write(f\"  - Image size: 512\\n\")\n",
        "    f.write(f\"  - Device: {device}\\n\")\n",
        "    f.write(f\"  - Optimizer: auto (AdamW)\\n\")\n",
        "    f.write(f\"  - Patience: 100\\n\\n\")\n",
        "\n",
        "    f.write(\"í”„ë¡œì íŠ¸ ì •ë³´:\\n\")\n",
        "    f.write(f\"  - Project: {args.project}\\n\")\n",
        "    f.write(f\"  - Name: {args.name}\\n\")\n",
        "    f.write(f\"  - Config: {args.config}\\n\\n\")\n",
        "\n",
        "    f.write(\"í´ë˜ìŠ¤ ì •ì˜:\\n\")\n",
        "    for name, cid in sorted(class_name_to_number.items(), key=lambda x: x[1]):\n",
        "        f.write(f\"  - {cid}: {name}\\n\")\n",
        "\n",
        "print(f\"âœ… í•™ìŠµ ì„¤ì • ì €ì¥ ì™„ë£Œ: training_config.txt\")\n",
        "\n",
        "# ========================================\n",
        "# 6. README ìƒì„±\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"6ï¸âƒ£ README ìƒì„±\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "readme_file = f\"{backup_root}/README.md\"\n",
        "with open(readme_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"# YOLOv11 Segmentation í•™ìŠµ ê²°ê³¼\\n\\n\")\n",
        "    f.write(f\"ìƒì„± ì¼ì‹œ: {timestamp}\\n\\n\")\n",
        "\n",
        "    f.write(\"## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°\\n\\n\")\n",
        "    f.write(\"```\\n\")\n",
        "    f.write(\"YOLOv11_Backup_{timestamp}/\\n\")\n",
        "    f.write(\"â”œâ”€â”€ training_results/          # í•™ìŠµ ê²°ê³¼\\n\")\n",
        "    f.write(\"â”‚   â”œâ”€â”€ weights/\\n\")\n",
        "    f.write(\"â”‚   â”‚   â”œâ”€â”€ best.pt           # ìµœì  ëª¨ë¸\\n\")\n",
        "    f.write(\"â”‚   â”‚   â””â”€â”€ last.pt           # ë§ˆì§€ë§‰ ëª¨ë¸\\n\")\n",
        "    f.write(\"â”‚   â”œâ”€â”€ results.png           # í•™ìŠµ ê³¡ì„ \\n\")\n",
        "    f.write(\"â”‚   â”œâ”€â”€ confusion_matrix.png  # Confusion Matrix\\n\")\n",
        "    f.write(\"â”‚   â””â”€â”€ results.csv           # ìƒì„¸ ê²°ê³¼\\n\")\n",
        "    f.write(\"â”œâ”€â”€ dataset_info/              # ë°ì´í„°ì…‹ ì •ë³´\\n\")\n",
        "    f.write(\"â”‚   â”œâ”€â”€ dataset.yaml          # YAML ì„¤ì •\\n\")\n",
        "    f.write(\"â”‚   â”œâ”€â”€ image_to_subdir.pkl   # ë§¤í•‘ ì •ë³´\\n\")\n",
        "    f.write(\"â”‚   â””â”€â”€ dataset_statistics.txt # í†µê³„\\n\")\n",
        "    f.write(\"â”œâ”€â”€ predictions/               # ì¶”ë¡  ê²°ê³¼ (ìˆëŠ” ê²½ìš°)\\n\")\n",
        "    f.write(\"â”œâ”€â”€ samples/                   # ìƒ˜í”Œ ë°ì´í„°\\n\")\n",
        "    f.write(\"â”‚   â”œâ”€â”€ train/\\n\")\n",
        "    f.write(\"â”‚   â”œâ”€â”€ val/\\n\")\n",
        "    f.write(\"â”‚   â””â”€â”€ test/\\n\")\n",
        "    f.write(\"â”œâ”€â”€ training_config.txt        # í•™ìŠµ ì„¤ì •\\n\")\n",
        "    f.write(\"â””â”€â”€ README.md                  # ì´ íŒŒì¼\\n\")\n",
        "    f.write(\"```\\n\\n\")\n",
        "\n",
        "    f.write(\"## ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´\\n\\n\")\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        img_count = len([f for f in os.listdir(f\"{dataset_root}/images/{split}\") if f.endswith('.jpg')])\n",
        "        label_count = len([f for f in os.listdir(f\"{dataset_root}/labels/{split}\") if f.endswith('.txt')])\n",
        "        match_rate = (label_count / img_count * 100) if img_count > 0 else 0\n",
        "        f.write(f\"- **{split.capitalize()}**: {img_count}ê°œ ì´ë¯¸ì§€, {label_count}ê°œ ë¼ë²¨ ({match_rate:.1f}%)\\n\")\n",
        "\n",
        "    f.write(\"\\n## ğŸ¯ í´ë˜ìŠ¤ ì •ë³´\\n\\n\")\n",
        "    f.write(\"| ID | í´ë˜ìŠ¤ëª… |\\n\")\n",
        "    f.write(\"|:--:|:--------|\\n\")\n",
        "    for name, cid in sorted(class_name_to_number.items(), key=lambda x: x[1]):\n",
        "        f.write(f\"| {cid} | {name} |\\n\")\n",
        "\n",
        "    f.write(\"\\n## ğŸš€ í•™ìŠµ ì„¤ì •\\n\\n\")\n",
        "    f.write(f\"- ëª¨ë¸: YOLOv11s-seg\\n\")\n",
        "    f.write(f\"- Epochs: {args.epoch}\\n\")\n",
        "    f.write(f\"- Batch size: 16\\n\")\n",
        "    f.write(f\"- Image size: 512Ã—512\\n\")\n",
        "    f.write(f\"- Device: {device}\\n\")\n",
        "\n",
        "    f.write(\"\\n## ğŸ“ ì¬í˜„ ë°©ë²•\\n\\n\")\n",
        "    f.write(\"### 1. ëª¨ë¸ ë¡œë“œ\\n\")\n",
        "    f.write(\"```python\\n\")\n",
        "    f.write(\"from ultralytics import YOLO\\n\\n\")\n",
        "    f.write(\"# ìµœì  ëª¨ë¸ ë¡œë“œ\\n\")\n",
        "    f.write(\"model = YOLO('training_results/weights/best.pt')\\n\")\n",
        "    f.write(\"```\\n\\n\")\n",
        "\n",
        "    f.write(\"### 2. ì¶”ë¡ \\n\")\n",
        "    f.write(\"```python\\n\")\n",
        "    f.write(\"# ì´ë¯¸ì§€ ì¶”ë¡ \\n\")\n",
        "    f.write(\"results = model.predict(\\n\")\n",
        "    f.write(\"    source='path/to/image.jpg',\\n\")\n",
        "    f.write(\"    imgsz=512,\\n\")\n",
        "    f.write(\"    conf=0.25,\\n\")\n",
        "    f.write(\"    device='cuda'\\n\")\n",
        "    f.write(\")\\n\")\n",
        "    f.write(\"```\\n\\n\")\n",
        "\n",
        "    f.write(\"### 3. ì¶”ê°€ í•™ìŠµ (Fine-tuning)\\n\")\n",
        "    f.write(\"```python\\n\")\n",
        "    f.write(\"# í•™ìŠµ ì¬ê°œ\\n\")\n",
        "    f.write(\"model = YOLO('training_results/weights/best.pt')\\n\")\n",
        "    f.write(\"model.train(\\n\")\n",
        "    f.write(\"    data='dataset_info/dataset.yaml',\\n\")\n",
        "    f.write(\"    epochs=50,\\n\")\n",
        "    f.write(\"    resume=True\\n\")\n",
        "    f.write(\")\\n\")\n",
        "    f.write(\"```\\n\\n\")\n",
        "\n",
        "    f.write(\"## ğŸ“§ ë¬¸ì˜\\n\\n\")\n",
        "    f.write(\"ì´ ëª¨ë¸ì— ëŒ€í•œ ë¬¸ì˜ì‚¬í•­ì€ í”„ë¡œì íŠ¸ ë‹´ë‹¹ìì—ê²Œ ì—°ë½í•˜ì„¸ìš”.\\n\")\n",
        "\n",
        "print(f\"âœ… README ìƒì„± ì™„ë£Œ: README.md\")\n",
        "\n",
        "# ========================================\n",
        "# 7. ë°±ì—… ìš”ì•½\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ“Š ë°±ì—… ìš”ì•½\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def get_dir_size(path):\n",
        "    \"\"\"ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚° (MB)\"\"\"\n",
        "    total = 0\n",
        "    try:\n",
        "        for dirpath, dirnames, filenames in os.walk(path):\n",
        "            for f in filenames:\n",
        "                fp = os.path.join(dirpath, f)\n",
        "                if os.path.exists(fp):\n",
        "                    total += os.path.getsize(fp)\n",
        "    except:\n",
        "        pass\n",
        "    return total / (1024**2)\n",
        "\n",
        "print(f\"\\nğŸ“ ë°±ì—… ìœ„ì¹˜: {backup_root}\\n\")\n",
        "\n",
        "backup_items = [\n",
        "    ('training_results', 'í•™ìŠµ ê²°ê³¼'),\n",
        "    ('dataset_info', 'ë°ì´í„°ì…‹ ì •ë³´'),\n",
        "    ('predictions', 'ì¶”ë¡  ê²°ê³¼'),\n",
        "    ('samples', 'ìƒ˜í”Œ ë°ì´í„°')\n",
        "]\n",
        "\n",
        "total_size = 0\n",
        "for item, desc in backup_items:\n",
        "    item_path = f\"{backup_root}/{item}\"\n",
        "    if os.path.exists(item_path):\n",
        "        size = get_dir_size(item_path)\n",
        "        total_size += size\n",
        "        print(f\"  âœ“ {desc:15s}: {size:7.2f} MB\")\n",
        "    else:\n",
        "        print(f\"  - {desc:15s}: ì—†ìŒ\")\n",
        "\n",
        "# ì„¤ì • íŒŒì¼ í¬ê¸°\n",
        "config_size = (os.path.getsize(config_file) + os.path.getsize(readme_file)) / (1024**2)\n",
        "total_size += config_size\n",
        "\n",
        "print(f\"  âœ“ {'ì„¤ì • íŒŒì¼':15s}: {config_size:7.2f} MB\")\n",
        "print(f\"  {'='*15}   {'='*7}\")\n",
        "print(f\"  {'ì´ í¬ê¸°':15s}: {total_size:7.2f} MB\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ëª¨ë“  ë°±ì—… ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nğŸ“‚ Google Driveì—ì„œ í™•ì¸:\")\n",
        "print(f\"   {backup_root.replace('/content/drive/RESULT/', '')}\") # ê²½ë¡œ ìˆ˜ì •\n",
        "print(\"\\nğŸ’¡ Tip: ë°±ì—…ëœ í´ë”ë¥¼ ì••ì¶•í•˜ë©´ ë” ì‰½ê²Œ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
